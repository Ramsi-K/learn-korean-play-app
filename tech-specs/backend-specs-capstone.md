# HagXwon Backend: Master Technical Specification (Bootcamp-Aligned)

This document defines the **backend architecture** for the HagXwon Capstone Project, aligned with GenAI Bootcamp requirements and extended with additional features, agents, and modular components.

---

## 1. Backend Framework & Philosophy

### ðŸ›  Bootcamp Minimum Requirements

The following are the core backend requirements specified by the GenAI Bootcamp:

- âœ… Must use **FastAPI** (or Flask if specified â€” this project uses FastAPI)
- âœ… Must use a **relational database** (SQLite3 minimum)
- âœ… Must expose a **working REST API** with JSON inputs/outputs
- âœ… Must be **Dockerized** for grading
- âœ… Must include **vocabulary endpoints** (e.g., `/words`, `/groups`)
- âœ… Must include **study session endpoints** (`/study_sessions`, `/review`)
- âœ… Must implement a **flashcard app** for language learning
- âœ… Must demonstrate **retrieval-augmented generation** (RAG) with basic working flow
- âœ… **No authentication system** required
- ðŸ” Fine-tuning is optional
- ðŸ” Frontend is optional, but backend must be API-accessible and testable via Swagger or curl

---

- **Framework:** FastAPI (finalized)
- **Bootcamp Requirements Met:**
  - Dockerized backend
  - REST API with multiple endpoints
  - Agent-based modularity
  - Vector DB usage (e.g., Chroma)
- **Project Additions:**
  - Multi-agent personality logic
  - Multi-vector-database structure
  - Flashcard + Tutor + MUD-based agent flows
  - Persona-driven RAG and prompt pipelines
  - JSONL-based fine-tuning support

---

## 2. Core Backend Structure

```
/backend
â”œâ”€â”€ main.py              # App entrypoint
â”œâ”€â”€ agents/              # Modular agents for different tasks
â”‚   â”œâ”€â”€ flashcard_agent.py
â”‚   â”œâ”€â”€ tutor_agent.py
â”‚   â”œâ”€â”€ ahjumma_agent.py
â”‚   â”œâ”€â”€ mud_agent.py
â”‚   â”œâ”€â”€ storybook_agent.py
â”‚   â”œâ”€â”€ listening_agent.py
â”‚   â”œâ”€â”€ noraebang_agent.py
â”‚   â””â”€â”€ object_label_agent.py
â”œâ”€â”€ rag/                 # Embedding + Retrieval modules
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ retrieve.py
â”œâ”€â”€ data/                # Data processing pipeline
â”‚   â”œâ”€â”€ pdf_to_text.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â””â”€â”€ metadata_tags.py
â”œâ”€â”€ db/                  # Traditional (SQLite) DB logic
â”‚   â”œâ”€â”€ schema.sql
â”‚   â””â”€â”€ user_data.py
â”œâ”€â”€ utils/               # Prompt templates and shared helpers
â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â””â”€â”€ config.py
â””â”€â”€ Dockerfile
```

---

## 3. Multi-Agent System Overview

Additional agents based on journal design updates:

| Agent            | Task                            | Input                           | Uses RAG | Vector DB               |
| ---------------- | ------------------------------- | ------------------------------- | -------- | ----------------------- |
| FlashcardAgent   | Vocab & grammar quizzing        | User's TOPIK level              | âœ…       | `grammar`               |
| TutorAgent       | Language explanation            | User question + persona         | âœ…       | `grammar`, `vocab`      |
| AhjummaAgent     | Judgy food-based feedback       | User input or image             | âœ…       | `culture`, `food`       |
| MudAgent         | Story game interaction          | Action verbs                    | âŒ       | (internal game logic)   |
| StorybookAgent   | Illustrated story generation    | Seed prompt, age group, persona | âœ…       | `narratives`, `grammar` |
| ListeningAgent   | Audio clip + comprehension quiz | Audio input                     | âœ…       | `grammar`, `vocab`      |
| NoraebangAgent   | Song-to-vocab and lyric parsing | Song name or lyrics             | âœ…       | `lyrics`, `vocab`       |
| ObjectLabelAgent | Real-time object name tutor     | Live webcam input               | âœ…       | `objects`, `culture`    |

(Note: These agents can be implemented incrementally. Core agents for submission are FlashcardAgent, TutorAgent, AhjummaAgent, and MudAgent.)

Each agent is autonomous but shares the retrieval and prompt pipeline layer.

---

## 4. Multi-VectorDB Strategy

We divide our data sources into two categories:

### ðŸ” Vector Databases (ChromaDB Collections)

These are embedded and queried via RAG:

```
/chroma
â”œâ”€â”€ grammar_db        â† Used by FlashcardAgent, TutorAgent
â”œâ”€â”€ culture_db        â† Used by AhjummaAgent, ObjectLabelAgent
â”œâ”€â”€ food_db           â† Used by AhjummaAgent (triggered by topic)
â”œâ”€â”€ vocab_db          â† Used by TutorAgent, ListeningAgent, NoraebangAgent
â”œâ”€â”€ narratives_db     â† Used by StorybookAgent (vectorized story elements)
â”œâ”€â”€ lyrics_db         â† Used by NoraebangAgent
â”œâ”€â”€ objects_db        â† Used by ObjectLabelAgent
```

Metadata per vector chunk includes:

- `source_type`: grammar, culture, lyrics, food, etc.
- `persona_affinity`: Ahjumma, Sunbae, etc.
- `TOPIK_level`
- `agent_affinity`: (e.g., StorybookAgent, NoraebangAgent)

### ðŸ—ƒ Static (Relational) Data (SQLite)

These are structured and managed in `/db/`:

- User profiles
- Vocabulary groups and word lists (used by `/words`, `/groups`)
- Study history and session scores (`/study_sessions`)
- Sentence prompt templates
- Flashcard progress and streaks
- Trainer persona definitions and metadata
- System logs, timestamps, and activity records

**Student activity** (progress, scores, session logs) is always stored in relational form. This ensures reproducibility, persistent tracking, and bootcamp compliance.

This separation ensures fast vector search while keeping structured relational logic clean and queryable. All agents retrieve vector context **but rely on static DBs for student state.**

---

## 5. Backend Flowchart

```mermaid
flowchart TD
  UI[Frontend UI]
  UI -->|POST /flashcard/next| FlashAPI[FlashcardAgent]
  UI -->|POST /tutor/ask| TutorAPI[TutorAgent]
  UI -->|POST /ahjumma/roast| RoastAPI[AhjummaAgent]
  UI -->|POST /mud/command| MudAPI[MudAgent]
  UI -->|POST /storybook/generate| StorybookAPI[StorybookAgent]
  UI -->|POST /listening/submit| ListenAPI[ListeningAgent]
  UI -->|POST /noraebang/lyrics| NoraebangAPI[NoraebangAgent]
  UI -->|POST /label/object| ObjectAPI[ObjectLabelAgent]

  FlashAPI --> RET1[Retrieve: grammar_db]
  TutorAPI --> RET2[Retrieve: grammar + vocab]
  RoastAPI --> RET3[Retrieve: culture + food]
  StorybookAPI --> RET4[Retrieve: narratives + grammar]
  ListenAPI --> RET5[Retrieve: vocab + grammar]
  NoraebangAPI --> RET6[Retrieve: lyrics + vocab]
  ObjectAPI --> RET7[Retrieve: objects + culture]

  RET1 --> LLM1[LLM]
  RET2 --> LLM2[LLM]
  RET3 --> LLM3[LLM]
  RET4 --> LLM4[LLM]
  RET5 --> LLM5[LLM]
  RET6 --> LLM6[LLM]
  RET7 --> LLM7[LLM]

  LLM1 --> UI
  LLM2 --> UI
  LLM3 --> UI
  LLM4 --> UI
  LLM5 --> UI
  LLM6 --> UI
  LLM7 --> UI
```

---

## 6. API Endpoint Specification (REST)

### ðŸ§© Bootcamp-Required Endpoints

These must be implemented and testable for final evaluation:

- `GET /words` â†’ List all vocabulary words
- `GET /groups` â†’ List vocabulary groups
- `GET /groups/{id}` â†’ Retrieve words in a group
- `POST /study_sessions` â†’ Begin new study session
- `POST /study_sessions/{id}/review` â†’ Submit flashcard/session response

### ðŸ”§ Custom Agent Endpoints

These reflect your added multi-agent functionality:

#### Flashcard Agent

- `POST /flashcard/next`
- `POST /flashcard/submit`

#### Tutor Agent

- `POST /tutor/ask`

```json
{
  "user_input": "How do I say 'I want to eat' in Korean?",
  "persona": "AhjummaGPT",
  "topik_level": 2
}
```

#### Ahjumma Agent

- `POST /ahjumma/roast`

```json
{
  "image_input": "base64string",
  "trigger": "handwriting"
}
```

#### MUD Agent

- `POST /mud/command`

```json
{
  "command": "LOOK"
}
```

#### Storybook Agent

- `POST /storybook/generate`

```json
{
  "prompt": "A fox and a tiger go on an adventure",
  "persona": "SunbaeGPT",
  "tone": "gentle"
}
```

#### Listening Agent

- `POST /listening/submit`

```json
{
  "audio_input": "base64string"
}
```

#### Noraebang Agent

- `POST /noraebang/lyrics`

```json
{
  "song_title": "Spring Day"
}
```

#### Object Label Agent

- `POST /label/object`

```json
{
  "image_input": "base64string"
}
```

---

## 7. Visual: Agent-DB Relationship

```mermaid
graph TD
  subgraph ChromaDB
    A[grammar_db]
    B[culture_db]
    C[food_db]
    D[narratives_db]
    E[vocab_db]
    F[lyrics_db]
    G[objects_db]
  end

  FlashcardAgent --> A
  TutorAgent --> A
  TutorAgent --> E
  AhjummaAgent --> B
  AhjummaAgent --> C
  StorybookAgent --> A
  StorybookAgent --> D
  ListeningAgent --> A
  ListeningAgent --> E
  NoraebangAgent --> F
  NoraebangAgent --> E
  ObjectLabelAgent --> G
  ObjectLabelAgent --> B
```

---

## 8. Fine-Tuning Interface (QLoRA)

| Field         | Description                        |
| ------------- | ---------------------------------- |
| `persona`     | Which trainer (Ahjumma, etc.)      |
| `tone`        | sass, polite, sarcastic            |
| `input`       | User message or prompt             |
| `response`    | Desired personality-grounded reply |
| `TOPIK_level` | Level relevance (1â€“6)              |

---

## 9. Deployment Stack (Bootcamp Compliant)

**Docker Compose setup:**

- `FastAPI` backend
- `ChromaDB` container
- Optional: `Ollama` container for local embedding/LLM

All packaged in single reproducible `docker-compose.yml`.

---

## 10. Notes & Bootcamp Alignment

âœ… All core agents are modular and testable via API\
âœ… REST-first architecture supports frontend integration\
âœ… Multi-vector database logic enables advanced RAG routing\
âœ… All data flow documented for reproducibility and evaluation

---
